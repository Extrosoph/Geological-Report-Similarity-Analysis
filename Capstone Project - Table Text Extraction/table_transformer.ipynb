{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "# Data processing and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Image processing and OCR\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "# Transformers and huggingface_hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import DetrFeatureExtractor, TableTransformerForObjectDetection\n",
    "\n",
    "# PyTorch\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_PATH = 'C:/Users/nick2/Desktop/Table Transformer/'\n",
    "TESSERACT_PATH = r'C:/Program Files/Tesseract-OCR/tesseract'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "test_labels = pd.read_csv(os.path.join(DATA_PATH, 'Labeled Data.csv'))\n",
    "test_labels['Pred'] = None\n",
    "\n",
    "# Set working directory\n",
    "working_directory = os.path.join(DATA_PATH, 'Test Dataset')\n",
    "os.chdir(working_directory)\n",
    "\n",
    "# Set tesseract path\n",
    "pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH\n",
    "\n",
    "# List all files and get their full paths\n",
    "files = os.listdir()\n",
    "concatenated_paths = [os.path.join(working_directory, filename) for filename in files]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Table Recognition (Nick, Theo, Kose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nick2\\anaconda3\\envs\\tables-detr\\lib\\site-packages\\transformers\\models\\detr\\feature_extraction_detr.py:28: FutureWarning: The class DetrFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DetrImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at microsoft/table-transformer-detection were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images in directory: 112461\n",
      "Processing images in directory: 112540\n",
      "Processing images in directory: 112697\n",
      "Processing images in directory: 112761\n",
      "Processing images in directory: 114158\n",
      "Processing images in directory: 114824\n",
      "Processing images in directory: 115109\n",
      "Processing images in directory: 127535\n",
      "Processing images in directory: 129866\n",
      "Processing images in directory: 130135\n",
      "Processing images in directory: 132241\n",
      "Processing images in directory: 132302\n",
      "Processing images in directory: 132577\n",
      "Processing images in directory: 134934\n",
      "Processing images in directory: 135347\n"
     ]
    }
   ],
   "source": [
    "VALID_EXTENSIONS = {'.jpg', '.png'}\n",
    "COLORS = [\n",
    "    [0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "    [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]\n",
    "]\n",
    "\n",
    "# Define Model and Feature Extractor\n",
    "feature_extractor = DetrFeatureExtractor()\n",
    "model = TableTransformerForObjectDetection.from_pretrained(\"microsoft/table-transformer-detection\")\n",
    "\n",
    "# Functions\n",
    "def get_valid_images(directory):\n",
    "    \"\"\"Get valid image files from a directory.\"\"\"\n",
    "    all_files = [os.path.join(directory, file) for file in os.listdir(directory)\n",
    "                 if os.path.isfile(os.path.join(directory, file))\n",
    "                 and os.path.splitext(file)[1].lower() in VALID_EXTENSIONS]\n",
    "    return all_files\n",
    "\n",
    "def process_directory(directory, plot=False):\n",
    "    \"\"\"Process all valid images in a directory and return a DataFrame.\"\"\"\n",
    "    print(f\"Processing images in directory: {os.path.basename(directory)}\")\n",
    "    \n",
    "    all_files = get_valid_images(directory)\n",
    "    all_data = []\n",
    "    \n",
    "    for image_file in all_files:\n",
    "        data = process_image(image_file, base_save_path=directory, plot=plot)  # provide base_save_path here\n",
    "        if data:\n",
    "            all_data.append(data)\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "def process_image(file_name, base_save_path, buffer=50, plot=True):\n",
    "    base_file_name = os.path.basename(file_name).rsplit('.', 1)[0]\n",
    "    file_parts = base_file_name.split('_')\n",
    "    \n",
    "    image = Image.open(file_name).convert(\"RGB\")\n",
    "    img_cv = cv2.imread(file_name)\n",
    "    width, height = image.size\n",
    "    encoding = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "\n",
    "    results = feature_extractor.post_process_object_detection(outputs, threshold=0.94, target_sizes=[(height, width)])[0]\n",
    "    num_tables = len(results['scores'])\n",
    "\n",
    "    boxes = results['boxes'] # Direct extraction from results\n",
    "    if len(boxes) > 0:\n",
    "        specific_save_path = os.path.join(base_save_path, f\"{base_file_name} Cropped Images\")\n",
    "        if not os.path.exists(specific_save_path):\n",
    "            os.makedirs(specific_save_path)\n",
    "\n",
    "        for index, box in enumerate(boxes):\n",
    "            x1 = int(box[0])\n",
    "            y1 = int(box[1])\n",
    "            x2 = int(box[2])\n",
    "            y2 = int(box[3])\n",
    "            x1 = max(0, x1 - buffer)\n",
    "            y1 = max(0, y1 - buffer)\n",
    "            x2 = min(img_cv.shape[1], x2 + buffer)\n",
    "            y2 = min(img_cv.shape[0], y2 + buffer)\n",
    "            \n",
    "            cropped_img = img_cv[y1:y2, x1:x2]\n",
    "            cv2.imwrite(f\"{specific_save_path}/{base_file_name}_table_{index}.png\", cropped_img)\n",
    "\n",
    "    if plot and num_tables > 0:\n",
    "        plot_results(image, results['scores'], results['labels'], results['boxes'])\n",
    "\n",
    "    if len(file_parts) == 2:\n",
    "        return {'FileName': file_parts[0], 'Page Number': file_parts[1], 'Number of tables': num_tables}\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def plot_results(pil_img, scores, labels, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for score, label, (xmin, ymin, xmax, ymax),c  in zip(scores.tolist(), labels.tolist(), boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        text = f'{model.config.id2label[label]}: {score:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "all_data = []\n",
    "\n",
    "for path in concatenated_paths:\n",
    "    if os.path.isdir(path):\n",
    "        df = process_directory(path, plot=False)\n",
    "        all_data.append(df)\n",
    "    elif os.path.isfile(path) and os.path.splitext(path)[1].lower() in VALID_EXTENSIONS:\n",
    "        parent_dir = os.path.dirname(path)  # This is where our cropped images would be saved\n",
    "        data = process_image(path, base_save_path=parent_dir, plot=False)\n",
    "        if data:\n",
    "            all_data.append(pd.DataFrame([data]))\n",
    "\n",
    "# Concatenate all data into the master dataframe\n",
    "master_df = pd.concat(all_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Evaluation of Table Prediction Accuracy (Theo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Perform Table Data Extraction Using Morphological Operations Tesseract (Nick, Jiyun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Extracted!\n",
      "Table Extracted!\n",
      "Table Extracted!\n",
      "Table Extracted!\n",
      "Table Extracted!\n",
      "Table Extracted!\n",
      "Table Extracted!\n",
      "Table Extracted!\n",
      "No Table Detected.\n",
      "Table Extracted!\n",
      "Table Extracted!\n",
      "No Table Detected.\n",
      "Table Extracted!\n",
      "Table Extracted!\n"
     ]
    }
   ],
   "source": [
    "def find_subdirs_with_name(path, keyword):\n",
    "    \"\"\"Return all subdirectories containing the given keyword.\"\"\"\n",
    "    return [os.path.join(root, directory) for root, dirs, files in os.walk(path) for directory in dirs if keyword in directory]\n",
    "\n",
    "def find_image_files_in_dir(path):\n",
    "    \"\"\"Return all image files in the directory.\"\"\"\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff']\n",
    "    return [os.path.join(root, file) for root, dirs, files in os.walk(path) for file in files if any(file.lower().endswith(ext) for ext in image_extensions)]\n",
    "\n",
    "\n",
    "def process_image(image_path):\n",
    "    image = cv2.imread(image_path, 0)\n",
    "    \n",
    "    img_bin = 255 - image\n",
    "    _, img_bin_otsu = cv2.threshold(img_bin, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Vertical Line extraction\n",
    "    vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, np.array(img_bin_otsu).shape[1]//150))\n",
    "    eroded_image = cv2.erode(img_bin_otsu, vertical_kernel, iterations=5)\n",
    "    vertical_lines = cv2.dilate(eroded_image, vertical_kernel, iterations=5)\n",
    "    \n",
    "    # Horizontal Line extraction\n",
    "    hor_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (np.array(img_bin_otsu).shape[1]//150, 1))\n",
    "    image_2 = cv2.erode(img_bin_otsu, hor_kernel, iterations=5)\n",
    "    horizontal_lines = cv2.dilate(image_2, hor_kernel, iterations=5)\n",
    "\n",
    "    # Combining\n",
    "    vertical_horizontal_lines = cv2.addWeighted(vertical_lines, 0.5, horizontal_lines, 0.5, 0)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    vertical_horizontal_lines = cv2.erode(~vertical_horizontal_lines, kernel, iterations=3)\n",
    "    _, vertical_horizontal_lines = cv2.threshold(vertical_horizontal_lines, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    b_image = cv2.bitwise_not(cv2.bitwise_xor(image, vertical_horizontal_lines))\n",
    "    return b_image, vertical_horizontal_lines\n",
    "\n",
    "\n",
    "def extract_bounding_boxes(b_image, vertical_horizontal_lines):\n",
    "    contours, _ = cv2.findContours(vertical_horizontal_lines, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    boundingBoxes = [cv2.boundingRect(contour) for contour in contours]\n",
    "    (contours, boundingBoxes) = zip(*sorted(zip(contours, boundingBoxes), key=lambda x: x[1][1]))\n",
    "\n",
    "    boxes = []\n",
    "    image_copy = b_image.copy()  # Initialize image_copy here\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        x-=1\n",
    "        w+=1\n",
    "        if w < 1000 and h < 500:\n",
    "            cv2.rectangle(image_copy, (x, y), (x + w, y + h), (0, 0, 255), 1)\n",
    "            boxes.append([x, y, w, h])\n",
    "            \n",
    "    # Uncomment Section to show plots\n",
    "    # plt.imshow(image_copy, cmap='gray')\n",
    "    # plt.title(\"Identified contours\")\n",
    "    # plt.show()\n",
    "    return boxes\n",
    "\n",
    "def extract_text_from_boxes(b_image, boxes):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    heights = [box[3] for box in boxes]  # Extracting the heights of all boxes\n",
    "    mean_height = np.mean(heights)\n",
    "\n",
    "    # Initialize columns list with the first box and set the previous box to the first box\n",
    "    columns.append(boxes[0])\n",
    "    previous_box = boxes[0]\n",
    "\n",
    "    for i in range(1, len(boxes)):\n",
    "        if boxes[i][1] <= previous_box[1] + mean_height / 2:\n",
    "            columns.append(boxes[i])\n",
    "            previous_box = boxes[i]\n",
    "            if i == len(boxes) - 1:\n",
    "                rows.append(columns)\n",
    "        else:\n",
    "            rows.append(columns)\n",
    "            columns = []\n",
    "            previous_box = boxes[i]\n",
    "            columns.append(boxes[i])\n",
    "\n",
    "    # Determine the total number of cells in the row with the maximum cells\n",
    "    total_cells = max([len(r) for r in rows])\n",
    "\n",
    "    # Find the center of each box in the first row\n",
    "    centers = [int(rows[0][j][0] + rows[0][j][2] / 2) for j in range(len(rows[0]))]\n",
    "    centers = np.array(centers)\n",
    "    centers.sort()\n",
    "\n",
    "    # Organize boxes by their closest center position\n",
    "    boxes_list = []\n",
    "    for i in range(len(rows)):\n",
    "        l = [[] for _ in range(total_cells)]\n",
    "        for j in range(len(rows[i])):\n",
    "            # Find the closest center for the current box\n",
    "            diff = abs(centers - (rows[i][j][0] + rows[i][j][2] / 4))\n",
    "            minimum = min(diff)\n",
    "            index = list(diff).index(minimum)\n",
    "            l[index].append(rows[i][j])\n",
    "        boxes_list.append(l)\n",
    "\n",
    "    # Extracting text from cells in the image\n",
    "    dataframe_final = []\n",
    "    for i in range(len(boxes_list)):\n",
    "        for j in range(len(boxes_list[i])):\n",
    "            s = ''\n",
    "            if len(boxes_list[i][j]) == 0:\n",
    "                dataframe_final.append(' ')\n",
    "            else:\n",
    "                for k in range(len(boxes_list[i][j])):\n",
    "                    x, y, w, h = boxes_list[i][j][k]\n",
    "                    roi = b_image[y:y+h, x:x+w]\n",
    "                    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 1))\n",
    "                    border = cv2.copyMakeBorder(roi, 2, 2, 2, 2, cv2.BORDER_CONSTANT, value=[255, 255])\n",
    "                    resizing = cv2.resize(border, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "                    dilation = cv2.dilate(resizing, kernel, iterations=1)\n",
    "                    erosion = cv2.erode(dilation, kernel, iterations=2)\n",
    "                    out = pytesseract.image_to_string(erosion).strip()\n",
    "                    s += \" \" + out\n",
    "                dataframe_final.append(s)\n",
    "\n",
    "    arr = np.array(dataframe_final)\n",
    "    dataframe = pd.DataFrame(arr.reshape(len(rows), total_cells))\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "def ocr_image_to_text(image_path, extracted_dir, counter):\n",
    "    b_image, vertical_horizontal_lines = process_image(image_path)\n",
    "    boxes = extract_bounding_boxes(b_image, vertical_horizontal_lines)\n",
    "\n",
    "    if len(boxes) <= 1:\n",
    "        print(\"No Table Detected.\")\n",
    "        return counter  # Return the current counter value\n",
    "    else:\n",
    "        print('Table Extracted!')\n",
    "        dataframe = extract_text_from_boxes(b_image, boxes)\n",
    "        \n",
    "        output_dir = os.path.dirname(extracted_dir)\n",
    "        if counter == 0:\n",
    "            csv_output_path = os.path.join(output_dir, \"output.csv\")\n",
    "        else:\n",
    "            csv_output_path = os.path.join(output_dir, f\"output{counter}.csv\")\n",
    "        \n",
    "        dataframe.to_csv(csv_output_path, index=False, header=False)\n",
    "\n",
    "        counter += 1  # Increment the counter\n",
    "    return counter  # Return the updated counter value\n",
    "\n",
    "def process_images_in_extracted_dirs(main_wd, output_dir):\n",
    "    global processed_tables_counter  # Declare the global variable here\n",
    "    cropped_dirs = find_subdirs_with_name(main_wd, \"Cropped\")\n",
    "    processed_tables_counter = 0 \n",
    "    for extracted_dir in cropped_dirs:\n",
    "        for image_path in find_image_files_in_dir(extracted_dir):\n",
    "            # Update the counter with the returned value\n",
    "            processed_tables_counter = ocr_image_to_text(image_path, extracted_dir, processed_tables_counter)\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    " # Introduce a global counter\n",
    "main_wd = r'C:\\Users\\nick2\\Desktop\\Table Transformer\\Test Dataset'\n",
    "output_directory = os.path.dirname(main_wd)\n",
    "process_images_in_extracted_dirs(main_wd, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Perform Table Data Extraction Using Machine Learning Techniques Tesseract (Kose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Extraction Evaluation (Jiyun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
